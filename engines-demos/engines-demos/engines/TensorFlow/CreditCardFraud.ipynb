{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Notebook to support tests for Keras engine based on Credit Card Fraud dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#pip install tensorflow==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ARTIFACTS_PATH = '../../artifacts/keras/'\n",
    "os.makedirs(ARTIFACTS_PATH, exist_ok=True) # Create path if not exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/creditcard/training.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target_value = 'Class'\n",
    "X_data = df.loc[:, df.columns != target_value].to_numpy()\n",
    "y_data = df[[target_value]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize dataset\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples in training data: 356 (0.18% of total)\n"
     ]
    }
   ],
   "source": [
    "# Analyse imbalanced targets\n",
    "counts = np.bincount(y_train[:, 0])\n",
    "print('Number of positive samples in training data: %d (%.2f%% of total)' % (\n",
    "    counts[1], 100 * float(counts[1]) / len(y_train)))\n",
    "\n",
    "weight_for_0 = 1.0 / counts[0]\n",
    "weight_for_1 = 1.0 / counts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Metrics to train the model\n",
    "metrics = [\n",
    "    keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "    keras.metrics.FalsePositives(name=\"fp\"),\n",
    "    keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "    keras.metrics.TruePositives(name=\"tp\"),\n",
    "    keras.metrics.Precision(name=\"precision\"),\n",
    "    keras.metrics.Recall(name=\"recall\"),\n",
    "]\n",
    "\n",
    "# Model optimizer\n",
    "optimizer = keras.optimizers.Adam(1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199364, 30)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               15872     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 163,713\n",
      "Trainable params: 163,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(512, activation='relu', input_shape=(X_train.shape[-1],)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ]\n",
    ")\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "98/98 - 1s - loss: 2.7746e-06 - fn: 41.0000 - fp: 25486.0000 - tn: 173522.0000 - tp: 315.0000 - precision: 0.0122 - recall: 0.8848 - val_loss: 0.1415 - val_fn: 12.0000 - val_fp: 2420.0000 - val_tn: 82887.0000 - val_tp: 124.0000 - val_precision: 0.0487 - val_recall: 0.9118\n",
      "Epoch 2/30\n",
      "98/98 - 1s - loss: 1.7050e-06 - fn: 29.0000 - fp: 8560.0000 - tn: 190448.0000 - tp: 327.0000 - precision: 0.0368 - recall: 0.9185 - val_loss: 0.1069 - val_fn: 11.0000 - val_fp: 3100.0000 - val_tn: 82207.0000 - val_tp: 125.0000 - val_precision: 0.0388 - val_recall: 0.9191\n",
      "Epoch 3/30\n",
      "98/98 - 1s - loss: 1.4856e-06 - fn: 27.0000 - fp: 9347.0000 - tn: 189661.0000 - tp: 329.0000 - precision: 0.0340 - recall: 0.9242 - val_loss: 0.1026 - val_fn: 9.0000 - val_fp: 2459.0000 - val_tn: 82848.0000 - val_tp: 127.0000 - val_precision: 0.0491 - val_recall: 0.9338\n",
      "Epoch 4/30\n",
      "98/98 - 1s - loss: 1.1997e-06 - fn: 23.0000 - fp: 7908.0000 - tn: 191100.0000 - tp: 333.0000 - precision: 0.0404 - recall: 0.9354 - val_loss: 0.0666 - val_fn: 11.0000 - val_fp: 1629.0000 - val_tn: 83678.0000 - val_tp: 125.0000 - val_precision: 0.0713 - val_recall: 0.9191\n",
      "Epoch 5/30\n",
      "98/98 - 1s - loss: 1.0709e-06 - fn: 19.0000 - fp: 7872.0000 - tn: 191136.0000 - tp: 337.0000 - precision: 0.0411 - recall: 0.9466 - val_loss: 0.0864 - val_fn: 7.0000 - val_fp: 2877.0000 - val_tn: 82430.0000 - val_tp: 129.0000 - val_precision: 0.0429 - val_recall: 0.9485\n",
      "Epoch 6/30\n",
      "98/98 - 1s - loss: 1.1325e-06 - fn: 17.0000 - fp: 7626.0000 - tn: 191382.0000 - tp: 339.0000 - precision: 0.0426 - recall: 0.9522 - val_loss: 0.1270 - val_fn: 9.0000 - val_fp: 3956.0000 - val_tn: 81351.0000 - val_tp: 127.0000 - val_precision: 0.0311 - val_recall: 0.9338\n",
      "Epoch 7/30\n",
      "98/98 - 1s - loss: 1.0539e-06 - fn: 15.0000 - fp: 7804.0000 - tn: 191204.0000 - tp: 341.0000 - precision: 0.0419 - recall: 0.9579 - val_loss: 0.1439 - val_fn: 5.0000 - val_fp: 5634.0000 - val_tn: 79673.0000 - val_tp: 131.0000 - val_precision: 0.0227 - val_recall: 0.9632\n",
      "Epoch 8/30\n",
      "98/98 - 1s - loss: 1.1127e-06 - fn: 18.0000 - fp: 9312.0000 - tn: 189696.0000 - tp: 338.0000 - precision: 0.0350 - recall: 0.9494 - val_loss: 0.0570 - val_fn: 10.0000 - val_fp: 1537.0000 - val_tn: 83770.0000 - val_tp: 126.0000 - val_precision: 0.0758 - val_recall: 0.9265\n",
      "Epoch 9/30\n",
      "98/98 - 1s - loss: 1.0405e-06 - fn: 18.0000 - fp: 8110.0000 - tn: 190898.0000 - tp: 338.0000 - precision: 0.0400 - recall: 0.9494 - val_loss: 0.0873 - val_fn: 5.0000 - val_fp: 3607.0000 - val_tn: 81700.0000 - val_tp: 131.0000 - val_precision: 0.0350 - val_recall: 0.9632\n",
      "Epoch 10/30\n",
      "98/98 - 1s - loss: 8.4215e-07 - fn: 12.0000 - fp: 6656.0000 - tn: 192352.0000 - tp: 344.0000 - precision: 0.0491 - recall: 0.9663 - val_loss: 0.0913 - val_fn: 7.0000 - val_fp: 3319.0000 - val_tn: 81988.0000 - val_tp: 129.0000 - val_precision: 0.0374 - val_recall: 0.9485\n",
      "Epoch 11/30\n",
      "98/98 - 1s - loss: 8.5968e-07 - fn: 9.0000 - fp: 8140.0000 - tn: 190868.0000 - tp: 347.0000 - precision: 0.0409 - recall: 0.9747 - val_loss: 0.0986 - val_fn: 10.0000 - val_fp: 1672.0000 - val_tn: 83635.0000 - val_tp: 126.0000 - val_precision: 0.0701 - val_recall: 0.9265\n",
      "Epoch 12/30\n",
      "98/98 - 1s - loss: 2.2643e-06 - fn: 29.0000 - fp: 8438.0000 - tn: 190570.0000 - tp: 327.0000 - precision: 0.0373 - recall: 0.9185 - val_loss: 0.1920 - val_fn: 12.0000 - val_fp: 764.0000 - val_tn: 84543.0000 - val_tp: 124.0000 - val_precision: 0.1396 - val_recall: 0.9118\n",
      "Epoch 13/30\n",
      "98/98 - 1s - loss: 1.8730e-06 - fn: 22.0000 - fp: 5482.0000 - tn: 193526.0000 - tp: 334.0000 - precision: 0.0574 - recall: 0.9382 - val_loss: 0.2198 - val_fn: 8.0000 - val_fp: 2276.0000 - val_tn: 83031.0000 - val_tp: 128.0000 - val_precision: 0.0532 - val_recall: 0.9412\n",
      "Epoch 14/30\n",
      "98/98 - 1s - loss: 1.1505e-06 - fn: 15.0000 - fp: 5108.0000 - tn: 193900.0000 - tp: 341.0000 - precision: 0.0626 - recall: 0.9579 - val_loss: 0.1136 - val_fn: 9.0000 - val_fp: 2944.0000 - val_tn: 82363.0000 - val_tp: 127.0000 - val_precision: 0.0414 - val_recall: 0.9338\n",
      "Epoch 15/30\n",
      "98/98 - 1s - loss: 1.9524e-06 - fn: 17.0000 - fp: 7291.0000 - tn: 191717.0000 - tp: 339.0000 - precision: 0.0444 - recall: 0.9522 - val_loss: 0.3500 - val_fn: 12.0000 - val_fp: 1007.0000 - val_tn: 84300.0000 - val_tp: 124.0000 - val_precision: 0.1096 - val_recall: 0.9118\n",
      "Epoch 16/30\n",
      "98/98 - 1s - loss: 1.4735e-06 - fn: 15.0000 - fp: 5035.0000 - tn: 193973.0000 - tp: 341.0000 - precision: 0.0634 - recall: 0.9579 - val_loss: 0.1593 - val_fn: 10.0000 - val_fp: 1787.0000 - val_tn: 83520.0000 - val_tp: 126.0000 - val_precision: 0.0659 - val_recall: 0.9265\n",
      "Epoch 17/30\n",
      "98/98 - 1s - loss: 1.5114e-06 - fn: 14.0000 - fp: 7752.0000 - tn: 191256.0000 - tp: 342.0000 - precision: 0.0423 - recall: 0.9607 - val_loss: 0.1600 - val_fn: 9.0000 - val_fp: 2902.0000 - val_tn: 82405.0000 - val_tp: 127.0000 - val_precision: 0.0419 - val_recall: 0.9338\n",
      "Epoch 18/30\n",
      "98/98 - 1s - loss: 1.0502e-06 - fn: 12.0000 - fp: 7074.0000 - tn: 191934.0000 - tp: 344.0000 - precision: 0.0464 - recall: 0.9663 - val_loss: 0.0973 - val_fn: 8.0000 - val_fp: 3098.0000 - val_tn: 82209.0000 - val_tp: 128.0000 - val_precision: 0.0397 - val_recall: 0.9412\n",
      "Epoch 19/30\n",
      "98/98 - 1s - loss: 7.1946e-07 - fn: 9.0000 - fp: 5065.0000 - tn: 193943.0000 - tp: 347.0000 - precision: 0.0641 - recall: 0.9747 - val_loss: 0.0629 - val_fn: 8.0000 - val_fp: 2242.0000 - val_tn: 83065.0000 - val_tp: 128.0000 - val_precision: 0.0540 - val_recall: 0.9412\n",
      "Epoch 20/30\n",
      "98/98 - 1s - loss: 7.9907e-07 - fn: 8.0000 - fp: 7017.0000 - tn: 191991.0000 - tp: 348.0000 - precision: 0.0473 - recall: 0.9775 - val_loss: 0.0646 - val_fn: 9.0000 - val_fp: 2267.0000 - val_tn: 83040.0000 - val_tp: 127.0000 - val_precision: 0.0530 - val_recall: 0.9338\n",
      "Epoch 21/30\n",
      "98/98 - 1s - loss: 6.7284e-07 - fn: 8.0000 - fp: 6188.0000 - tn: 192820.0000 - tp: 348.0000 - precision: 0.0532 - recall: 0.9775 - val_loss: 0.0383 - val_fn: 10.0000 - val_fp: 1011.0000 - val_tn: 84296.0000 - val_tp: 126.0000 - val_precision: 0.1108 - val_recall: 0.9265\n",
      "Epoch 22/30\n",
      "98/98 - 1s - loss: 4.4835e-07 - fn: 4.0000 - fp: 4245.0000 - tn: 194763.0000 - tp: 352.0000 - precision: 0.0766 - recall: 0.9888 - val_loss: 0.0340 - val_fn: 12.0000 - val_fp: 965.0000 - val_tn: 84342.0000 - val_tp: 124.0000 - val_precision: 0.1139 - val_recall: 0.9118\n",
      "Epoch 23/30\n",
      "98/98 - 1s - loss: 8.8400e-07 - fn: 9.0000 - fp: 7454.0000 - tn: 191554.0000 - tp: 347.0000 - precision: 0.0445 - recall: 0.9747 - val_loss: 0.0786 - val_fn: 9.0000 - val_fp: 3842.0000 - val_tn: 81465.0000 - val_tp: 127.0000 - val_precision: 0.0320 - val_recall: 0.9338\n",
      "Epoch 24/30\n",
      "98/98 - 1s - loss: 5.5701e-07 - fn: 3.0000 - fp: 6286.0000 - tn: 192722.0000 - tp: 353.0000 - precision: 0.0532 - recall: 0.9916 - val_loss: 0.0593 - val_fn: 8.0000 - val_fp: 2730.0000 - val_tn: 82577.0000 - val_tp: 128.0000 - val_precision: 0.0448 - val_recall: 0.9412\n",
      "Epoch 25/30\n",
      "98/98 - 1s - loss: 5.7303e-07 - fn: 5.0000 - fp: 5520.0000 - tn: 193488.0000 - tp: 351.0000 - precision: 0.0598 - recall: 0.9860 - val_loss: 0.0739 - val_fn: 9.0000 - val_fp: 2942.0000 - val_tn: 82365.0000 - val_tp: 127.0000 - val_precision: 0.0414 - val_recall: 0.9338\n",
      "Epoch 26/30\n",
      "98/98 - 1s - loss: 6.0987e-07 - fn: 8.0000 - fp: 5633.0000 - tn: 193375.0000 - tp: 348.0000 - precision: 0.0582 - recall: 0.9775 - val_loss: 0.0762 - val_fn: 11.0000 - val_fp: 2631.0000 - val_tn: 82676.0000 - val_tp: 125.0000 - val_precision: 0.0454 - val_recall: 0.9191\n",
      "Epoch 27/30\n",
      "98/98 - 1s - loss: 5.1240e-07 - fn: 6.0000 - fp: 4571.0000 - tn: 194437.0000 - tp: 350.0000 - precision: 0.0711 - recall: 0.9831 - val_loss: 0.0630 - val_fn: 9.0000 - val_fp: 2794.0000 - val_tn: 82513.0000 - val_tp: 127.0000 - val_precision: 0.0435 - val_recall: 0.9338\n",
      "Epoch 28/30\n",
      "98/98 - 1s - loss: 4.4384e-07 - fn: 1.0000 - fp: 5389.0000 - tn: 193619.0000 - tp: 355.0000 - precision: 0.0618 - recall: 0.9972 - val_loss: 0.0369 - val_fn: 11.0000 - val_fp: 1062.0000 - val_tn: 84245.0000 - val_tp: 125.0000 - val_precision: 0.1053 - val_recall: 0.9191\n",
      "Epoch 29/30\n",
      "98/98 - 1s - loss: 5.4478e-07 - fn: 4.0000 - fp: 4200.0000 - tn: 194808.0000 - tp: 352.0000 - precision: 0.0773 - recall: 0.9888 - val_loss: 0.0967 - val_fn: 9.0000 - val_fp: 3749.0000 - val_tn: 81558.0000 - val_tp: 127.0000 - val_precision: 0.0328 - val_recall: 0.9338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n",
      "98/98 - 1s - loss: 6.8207e-07 - fn: 4.0000 - fp: 6024.0000 - tn: 192984.0000 - tp: 352.0000 - precision: 0.0552 - recall: 0.9888 - val_loss: 0.0612 - val_fn: 9.0000 - val_fp: 1578.0000 - val_tn: 83729.0000 - val_tp: 127.0000 - val_precision: 0.0745 - val_recall: 0.9338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f06c7d49700>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=2048,\n",
    "    epochs=30,\n",
    "    verbose=2,\n",
    "    validation_data=(X_test, y_test),\n",
    "    class_weight={0: weight_for_0, 1: weight_for_1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save(ARTIFACTS_PATH + 'keras_credit_card_fraud_sequential.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load model and make sample prediction\n",
    "model = tf.keras.models.load_model(ARTIFACTS_PATH + 'keras_credit_card_fraud_sequential.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.5477624e-14]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[0.52941, 0.36364, 0.64286, 0.45833, 0.52941, 0.36364, 0.64286, 0.45833, 0.52941, 0.36364,\n",
    "      0.52941, 0.36364, 0.64286, 0.45833, 0.52941, 0.36364, 0.64286, 0.45833, 0.52941, 0.36364,\n",
    "      0.52941, 0.36364, 0.64286, 0.45833, 0.52941, 0.36364, 0.64286, 0.45833, 0.52941, 0.36364]]\n",
    "model.predict(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Non-sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               15872     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 163,713\n",
      "Trainable params: 163,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x_input = keras.layers.Input(shape=(X_train.shape[-1],))\n",
    "\n",
    "x = x_input\n",
    "x = keras.layers.Dense(512, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Dense(256, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Dense(64, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "x_output = x\n",
    "\n",
    "model = keras.Model(inputs=x_input, outputs=x_output)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "98/98 - 1s - loss: 2.4854e-06 - fn: 49.0000 - fp: 15845.0000 - tn: 268470.0000 - tp: 443.0000 - precision: 0.0272 - recall: 0.9004 - val_loss: 0.1718 - val_fn: 13.0000 - val_fp: 1178.0000 - val_tn: 84129.0000 - val_tp: 123.0000 - val_precision: 0.0945 - val_recall: 0.9044\n",
      "Epoch 2/30\n",
      "98/98 - 1s - loss: 1.9279e-06 - fn: 34.0000 - fp: 7957.0000 - tn: 191051.0000 - tp: 322.0000 - precision: 0.0389 - recall: 0.9045 - val_loss: 0.2467 - val_fn: 16.0000 - val_fp: 3202.0000 - val_tn: 82105.0000 - val_tp: 120.0000 - val_precision: 0.0361 - val_recall: 0.8824\n",
      "Epoch 3/30\n",
      "98/98 - 1s - loss: 1.5695e-06 - fn: 32.0000 - fp: 6047.0000 - tn: 192961.0000 - tp: 324.0000 - precision: 0.0509 - recall: 0.9101 - val_loss: 0.1276 - val_fn: 10.0000 - val_fp: 2873.0000 - val_tn: 82434.0000 - val_tp: 126.0000 - val_precision: 0.0420 - val_recall: 0.9265\n",
      "Epoch 4/30\n",
      "98/98 - 1s - loss: 1.1139e-06 - fn: 21.0000 - fp: 6433.0000 - tn: 192575.0000 - tp: 335.0000 - precision: 0.0495 - recall: 0.9410 - val_loss: 0.1038 - val_fn: 10.0000 - val_fp: 1827.0000 - val_tn: 83480.0000 - val_tp: 126.0000 - val_precision: 0.0645 - val_recall: 0.9265\n",
      "Epoch 5/30\n",
      "98/98 - 1s - loss: 1.1823e-06 - fn: 22.0000 - fp: 6102.0000 - tn: 192906.0000 - tp: 334.0000 - precision: 0.0519 - recall: 0.9382 - val_loss: 0.0787 - val_fn: 10.0000 - val_fp: 1592.0000 - val_tn: 83715.0000 - val_tp: 126.0000 - val_precision: 0.0733 - val_recall: 0.9265\n",
      "Epoch 6/30\n",
      "98/98 - 1s - loss: 1.1656e-06 - fn: 23.0000 - fp: 6452.0000 - tn: 192556.0000 - tp: 333.0000 - precision: 0.0491 - recall: 0.9354 - val_loss: 0.2365 - val_fn: 8.0000 - val_fp: 7301.0000 - val_tn: 78006.0000 - val_tp: 128.0000 - val_precision: 0.0172 - val_recall: 0.9412\n",
      "Epoch 7/30\n",
      "98/98 - 1s - loss: 1.3082e-06 - fn: 24.0000 - fp: 6635.0000 - tn: 192373.0000 - tp: 332.0000 - precision: 0.0477 - recall: 0.9326 - val_loss: 0.2500 - val_fn: 9.0000 - val_fp: 6685.0000 - val_tn: 78622.0000 - val_tp: 127.0000 - val_precision: 0.0186 - val_recall: 0.9338\n",
      "Epoch 8/30\n",
      "98/98 - 1s - loss: 1.4172e-06 - fn: 18.0000 - fp: 7292.0000 - tn: 191716.0000 - tp: 338.0000 - precision: 0.0443 - recall: 0.9494 - val_loss: 0.2488 - val_fn: 9.0000 - val_fp: 2713.0000 - val_tn: 82594.0000 - val_tp: 127.0000 - val_precision: 0.0447 - val_recall: 0.9338\n",
      "Epoch 9/30\n",
      "98/98 - 1s - loss: 1.2773e-06 - fn: 26.0000 - fp: 5118.0000 - tn: 193890.0000 - tp: 330.0000 - precision: 0.0606 - recall: 0.9270 - val_loss: 0.0974 - val_fn: 11.0000 - val_fp: 1956.0000 - val_tn: 83351.0000 - val_tp: 125.0000 - val_precision: 0.0601 - val_recall: 0.9191\n",
      "Epoch 10/30\n",
      "98/98 - 1s - loss: 8.8630e-07 - fn: 16.0000 - fp: 5472.0000 - tn: 193536.0000 - tp: 340.0000 - precision: 0.0585 - recall: 0.9551 - val_loss: 0.0783 - val_fn: 8.0000 - val_fp: 2013.0000 - val_tn: 83294.0000 - val_tp: 128.0000 - val_precision: 0.0598 - val_recall: 0.9412\n",
      "Epoch 11/30\n",
      "98/98 - 1s - loss: 7.3274e-07 - fn: 9.0000 - fp: 4958.0000 - tn: 194050.0000 - tp: 347.0000 - precision: 0.0654 - recall: 0.9747 - val_loss: 0.0933 - val_fn: 11.0000 - val_fp: 2453.0000 - val_tn: 82854.0000 - val_tp: 125.0000 - val_precision: 0.0485 - val_recall: 0.9191\n",
      "Epoch 12/30\n",
      "98/98 - 1s - loss: 9.2142e-07 - fn: 13.0000 - fp: 5539.0000 - tn: 193469.0000 - tp: 343.0000 - precision: 0.0583 - recall: 0.9635 - val_loss: 0.0859 - val_fn: 10.0000 - val_fp: 2280.0000 - val_tn: 83027.0000 - val_tp: 126.0000 - val_precision: 0.0524 - val_recall: 0.9265\n",
      "Epoch 13/30\n",
      "98/98 - 1s - loss: 7.7857e-07 - fn: 11.0000 - fp: 6382.0000 - tn: 192626.0000 - tp: 345.0000 - precision: 0.0513 - recall: 0.9691 - val_loss: 0.0565 - val_fn: 9.0000 - val_fp: 2013.0000 - val_tn: 83294.0000 - val_tp: 127.0000 - val_precision: 0.0593 - val_recall: 0.9338\n",
      "Epoch 14/30\n",
      "98/98 - 1s - loss: 1.0120e-06 - fn: 11.0000 - fp: 7556.0000 - tn: 191452.0000 - tp: 345.0000 - precision: 0.0437 - recall: 0.9691 - val_loss: 0.1018 - val_fn: 13.0000 - val_fp: 1765.0000 - val_tn: 83542.0000 - val_tp: 123.0000 - val_precision: 0.0651 - val_recall: 0.9044\n",
      "Epoch 15/30\n",
      "98/98 - 1s - loss: 8.1556e-07 - fn: 10.0000 - fp: 5547.0000 - tn: 193461.0000 - tp: 346.0000 - precision: 0.0587 - recall: 0.9719 - val_loss: 0.0623 - val_fn: 12.0000 - val_fp: 1665.0000 - val_tn: 83642.0000 - val_tp: 124.0000 - val_precision: 0.0693 - val_recall: 0.9118\n",
      "Epoch 16/30\n",
      "98/98 - 1s - loss: 7.5462e-07 - fn: 12.0000 - fp: 5254.0000 - tn: 193754.0000 - tp: 344.0000 - precision: 0.0615 - recall: 0.9663 - val_loss: 0.1341 - val_fn: 7.0000 - val_fp: 4692.0000 - val_tn: 80615.0000 - val_tp: 129.0000 - val_precision: 0.0268 - val_recall: 0.9485\n",
      "Epoch 17/30\n",
      "98/98 - 1s - loss: 6.8696e-07 - fn: 5.0000 - fp: 5215.0000 - tn: 193793.0000 - tp: 351.0000 - precision: 0.0631 - recall: 0.9860 - val_loss: 0.1102 - val_fn: 9.0000 - val_fp: 3504.0000 - val_tn: 81803.0000 - val_tp: 127.0000 - val_precision: 0.0350 - val_recall: 0.9338\n",
      "Epoch 18/30\n",
      "98/98 - 1s - loss: 6.2502e-07 - fn: 5.0000 - fp: 5239.0000 - tn: 193769.0000 - tp: 351.0000 - precision: 0.0628 - recall: 0.9860 - val_loss: 0.0703 - val_fn: 8.0000 - val_fp: 2157.0000 - val_tn: 83150.0000 - val_tp: 128.0000 - val_precision: 0.0560 - val_recall: 0.9412\n",
      "Epoch 19/30\n",
      "98/98 - 1s - loss: 7.1076e-07 - fn: 6.0000 - fp: 4336.0000 - tn: 194672.0000 - tp: 350.0000 - precision: 0.0747 - recall: 0.9831 - val_loss: 0.0617 - val_fn: 11.0000 - val_fp: 1553.0000 - val_tn: 83754.0000 - val_tp: 125.0000 - val_precision: 0.0745 - val_recall: 0.9191\n",
      "Epoch 20/30\n",
      "98/98 - 1s - loss: 5.4315e-07 - fn: 6.0000 - fp: 4118.0000 - tn: 194890.0000 - tp: 350.0000 - precision: 0.0783 - recall: 0.9831 - val_loss: 0.0832 - val_fn: 9.0000 - val_fp: 2400.0000 - val_tn: 82907.0000 - val_tp: 127.0000 - val_precision: 0.0503 - val_recall: 0.9338\n",
      "Epoch 21/30\n",
      "98/98 - 1s - loss: 6.1991e-07 - fn: 7.0000 - fp: 5173.0000 - tn: 193835.0000 - tp: 349.0000 - precision: 0.0632 - recall: 0.9803 - val_loss: 0.0844 - val_fn: 10.0000 - val_fp: 3290.0000 - val_tn: 82017.0000 - val_tp: 126.0000 - val_precision: 0.0369 - val_recall: 0.9265\n",
      "Epoch 22/30\n",
      "98/98 - 1s - loss: 5.0074e-07 - fn: 5.0000 - fp: 4102.0000 - tn: 194906.0000 - tp: 351.0000 - precision: 0.0788 - recall: 0.9860 - val_loss: 0.0432 - val_fn: 10.0000 - val_fp: 1294.0000 - val_tn: 84013.0000 - val_tp: 126.0000 - val_precision: 0.0887 - val_recall: 0.9265\n",
      "Epoch 23/30\n",
      "98/98 - 1s - loss: 4.3486e-07 - fn: 5.0000 - fp: 3076.0000 - tn: 195932.0000 - tp: 351.0000 - precision: 0.1024 - recall: 0.9860 - val_loss: 0.0642 - val_fn: 9.0000 - val_fp: 1983.0000 - val_tn: 83324.0000 - val_tp: 127.0000 - val_precision: 0.0602 - val_recall: 0.9338\n",
      "Epoch 24/30\n",
      "98/98 - 1s - loss: 4.4429e-07 - fn: 3.0000 - fp: 4386.0000 - tn: 194622.0000 - tp: 353.0000 - precision: 0.0745 - recall: 0.9916 - val_loss: 0.0267 - val_fn: 12.0000 - val_fp: 746.0000 - val_tn: 84561.0000 - val_tp: 124.0000 - val_precision: 0.1425 - val_recall: 0.9118\n",
      "Epoch 25/30\n",
      "98/98 - 1s - loss: 4.7436e-07 - fn: 5.0000 - fp: 3312.0000 - tn: 195696.0000 - tp: 351.0000 - precision: 0.0958 - recall: 0.9860 - val_loss: 0.0665 - val_fn: 10.0000 - val_fp: 2170.0000 - val_tn: 83137.0000 - val_tp: 126.0000 - val_precision: 0.0549 - val_recall: 0.9265\n",
      "Epoch 26/30\n",
      "98/98 - 1s - loss: 5.2350e-07 - fn: 3.0000 - fp: 4838.0000 - tn: 194170.0000 - tp: 353.0000 - precision: 0.0680 - recall: 0.9916 - val_loss: 0.0588 - val_fn: 13.0000 - val_fp: 1544.0000 - val_tn: 83763.0000 - val_tp: 123.0000 - val_precision: 0.0738 - val_recall: 0.9044\n",
      "Epoch 27/30\n",
      "98/98 - 1s - loss: 4.9162e-07 - fn: 5.0000 - fp: 4107.0000 - tn: 194901.0000 - tp: 351.0000 - precision: 0.0787 - recall: 0.9860 - val_loss: 0.0397 - val_fn: 10.0000 - val_fp: 1316.0000 - val_tn: 83991.0000 - val_tp: 126.0000 - val_precision: 0.0874 - val_recall: 0.9265\n",
      "Epoch 28/30\n",
      "98/98 - 1s - loss: 2.7798e-07 - fn: 1.0000 - fp: 2664.0000 - tn: 196344.0000 - tp: 355.0000 - precision: 0.1176 - recall: 0.9972 - val_loss: 0.0275 - val_fn: 12.0000 - val_fp: 800.0000 - val_tn: 84507.0000 - val_tp: 124.0000 - val_precision: 0.1342 - val_recall: 0.9118\n",
      "Epoch 29/30\n",
      "98/98 - 1s - loss: 2.8536e-07 - fn: 3.0000 - fp: 2095.0000 - tn: 196913.0000 - tp: 353.0000 - precision: 0.1442 - recall: 0.9916 - val_loss: 0.0694 - val_fn: 13.0000 - val_fp: 1512.0000 - val_tn: 83795.0000 - val_tp: 123.0000 - val_precision: 0.0752 - val_recall: 0.9044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n",
      "98/98 - 1s - loss: 4.6947e-07 - fn: 4.0000 - fp: 4719.0000 - tn: 194289.0000 - tp: 352.0000 - precision: 0.0694 - recall: 0.9888 - val_loss: 0.0292 - val_fn: 13.0000 - val_fp: 871.0000 - val_tn: 84436.0000 - val_tp: 123.0000 - val_precision: 0.1237 - val_recall: 0.9044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f06b8499430>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=2048,\n",
    "    epochs=30,\n",
    "    verbose=2,\n",
    "    validation_data=(X_test, y_test),\n",
    "    class_weight={0: weight_for_0, 1: weight_for_1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save(ARTIFACTS_PATH + 'keras_credit_card_fraud_nonsequential.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Non-sequential model with branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 512)          15872       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 512)          0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          131328      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 256)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 128)          32896       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 128)          65664       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 128)          0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 128)          0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            257         concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 246,017\n",
      "Trainable params: 246,017\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x_input = keras.layers.Input(shape=(X_train.shape[-1],))\n",
    "\n",
    "x = x_input\n",
    "x = keras.layers.Dense(512, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "x_1 = x\n",
    "x_1 = keras.layers.Dense(256, activation='relu')(x_1)\n",
    "x_1 = keras.layers.Dropout(0.2)(x_1)\n",
    "x_1 = keras.layers.Dense(128, activation='relu')(x_1)\n",
    "x_1 = keras.layers.Dropout(0.2)(x_1)\n",
    "\n",
    "x_2 = x\n",
    "x_2 = keras.layers.Dense(128, activation='relu')(x_2)\n",
    "x_2 = keras.layers.Dropout(0.2)(x_2)\n",
    "\n",
    "x = keras.layers.Concatenate(axis=-1)([x_1, x_2])\n",
    "x = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "x_output = x\n",
    "\n",
    "model = keras.Model(inputs=x_input, outputs=x_output)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "98/98 - 2s - loss: 2.4585e-06 - fn: 55.0000 - fp: 16211.0000 - tn: 268104.0000 - tp: 437.0000 - precision: 0.0262 - recall: 0.8882 - val_loss: 0.1093 - val_fn: 11.0000 - val_fp: 2618.0000 - val_tn: 82689.0000 - val_tp: 125.0000 - val_precision: 0.0456 - val_recall: 0.9191\n",
      "Epoch 2/30\n",
      "98/98 - 1s - loss: 2.6098e-06 - fn: 32.0000 - fp: 9213.0000 - tn: 189795.0000 - tp: 324.0000 - precision: 0.0340 - recall: 0.9101 - val_loss: 0.7875 - val_fn: 4.0000 - val_fp: 18608.0000 - val_tn: 66699.0000 - val_tp: 132.0000 - val_precision: 0.0070 - val_recall: 0.9706\n",
      "Epoch 3/30\n",
      "98/98 - 1s - loss: 2.6002e-06 - fn: 29.0000 - fp: 10468.0000 - tn: 188540.0000 - tp: 327.0000 - precision: 0.0303 - recall: 0.9185 - val_loss: 0.3203 - val_fn: 9.0000 - val_fp: 5013.0000 - val_tn: 80294.0000 - val_tp: 127.0000 - val_precision: 0.0247 - val_recall: 0.9338\n",
      "Epoch 4/30\n",
      "98/98 - 1s - loss: 1.5044e-06 - fn: 25.0000 - fp: 5725.0000 - tn: 193283.0000 - tp: 331.0000 - precision: 0.0547 - recall: 0.9298 - val_loss: 0.1074 - val_fn: 9.0000 - val_fp: 2144.0000 - val_tn: 83163.0000 - val_tp: 127.0000 - val_precision: 0.0559 - val_recall: 0.9338\n",
      "Epoch 5/30\n",
      "98/98 - 1s - loss: 2.2731e-06 - fn: 24.0000 - fp: 8068.0000 - tn: 190940.0000 - tp: 332.0000 - precision: 0.0395 - recall: 0.9326 - val_loss: 0.3970 - val_fn: 9.0000 - val_fp: 5391.0000 - val_tn: 79916.0000 - val_tp: 127.0000 - val_precision: 0.0230 - val_recall: 0.9338\n",
      "Epoch 6/30\n",
      "98/98 - 1s - loss: 1.5636e-06 - fn: 23.0000 - fp: 6962.0000 - tn: 192046.0000 - tp: 333.0000 - precision: 0.0456 - recall: 0.9354 - val_loss: 0.1461 - val_fn: 8.0000 - val_fp: 2874.0000 - val_tn: 82433.0000 - val_tp: 128.0000 - val_precision: 0.0426 - val_recall: 0.9412\n",
      "Epoch 7/30\n",
      "98/98 - 1s - loss: 1.8915e-06 - fn: 16.0000 - fp: 4938.0000 - tn: 194070.0000 - tp: 340.0000 - precision: 0.0644 - recall: 0.9551 - val_loss: 0.1976 - val_fn: 12.0000 - val_fp: 1443.0000 - val_tn: 83864.0000 - val_tp: 124.0000 - val_precision: 0.0791 - val_recall: 0.9118\n",
      "Epoch 8/30\n",
      "98/98 - 1s - loss: 1.3629e-06 - fn: 14.0000 - fp: 5113.0000 - tn: 193895.0000 - tp: 342.0000 - precision: 0.0627 - recall: 0.9607 - val_loss: 0.3417 - val_fn: 7.0000 - val_fp: 4382.0000 - val_tn: 80925.0000 - val_tp: 129.0000 - val_precision: 0.0286 - val_recall: 0.9485\n",
      "Epoch 9/30\n",
      "98/98 - 1s - loss: 1.5655e-06 - fn: 17.0000 - fp: 5915.0000 - tn: 193093.0000 - tp: 339.0000 - precision: 0.0542 - recall: 0.9522 - val_loss: 0.3717 - val_fn: 8.0000 - val_fp: 5725.0000 - val_tn: 79582.0000 - val_tp: 128.0000 - val_precision: 0.0219 - val_recall: 0.9412\n",
      "Epoch 10/30\n",
      "98/98 - 1s - loss: 1.4024e-06 - fn: 16.0000 - fp: 6370.0000 - tn: 192638.0000 - tp: 340.0000 - precision: 0.0507 - recall: 0.9551 - val_loss: 0.1436 - val_fn: 8.0000 - val_fp: 2633.0000 - val_tn: 82674.0000 - val_tp: 128.0000 - val_precision: 0.0464 - val_recall: 0.9412\n",
      "Epoch 11/30\n",
      "98/98 - 1s - loss: 9.9643e-07 - fn: 12.0000 - fp: 4279.0000 - tn: 194729.0000 - tp: 344.0000 - precision: 0.0744 - recall: 0.9663 - val_loss: 0.7350 - val_fn: 8.0000 - val_fp: 5157.0000 - val_tn: 80150.0000 - val_tp: 128.0000 - val_precision: 0.0242 - val_recall: 0.9412\n",
      "Epoch 12/30\n",
      "98/98 - 1s - loss: 2.5388e-06 - fn: 11.0000 - fp: 6625.0000 - tn: 192383.0000 - tp: 345.0000 - precision: 0.0495 - recall: 0.9691 - val_loss: 0.2477 - val_fn: 13.0000 - val_fp: 1496.0000 - val_tn: 83811.0000 - val_tp: 123.0000 - val_precision: 0.0760 - val_recall: 0.9044\n",
      "Epoch 13/30\n",
      "98/98 - 1s - loss: 2.0951e-06 - fn: 13.0000 - fp: 4816.0000 - tn: 194192.0000 - tp: 343.0000 - precision: 0.0665 - recall: 0.9635 - val_loss: 0.3485 - val_fn: 12.0000 - val_fp: 1439.0000 - val_tn: 83868.0000 - val_tp: 124.0000 - val_precision: 0.0793 - val_recall: 0.9118\n",
      "Epoch 14/30\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=2048,\n",
    "    epochs=30,\n",
    "    verbose=2,\n",
    "    validation_data=(X_test, y_test),\n",
    "    class_weight={0: weight_for_0, 1: weight_for_1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save(ARTIFACTS_PATH + 'keras_credit_card_fraud_nonsequential_branch.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sample prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a = [[0.52941, 0.36364, 0.64286, 0.45833, 0.52941, 0.36364, 0.64286, 0.45833, 0.52941, 0.36364,\n",
    "      0.52941, 0.36364, 0.64286, 0.45833, 0.52941, 0.36364, 0.64286, 0.45833, 0.52941, 0.36364,\n",
    "      0.52941, 0.36364, 0.64286, 0.45833, 0.52941, 0.36364, 0.64286, 0.45833, 0.52941, 0.36364]]\n",
    "model.predict(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}